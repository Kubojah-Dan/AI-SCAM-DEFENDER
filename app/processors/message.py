import pandas as pd
import numpy as np
import re
import math
import warnings
from urllib.parse import urlparse
from tqdm import tqdm
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.utils.class_weight import compute_class_weight
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from google.colab import files

warnings.filterwarnings('ignore')
print("‚úÖ XGBoost version:", xgb.__version__)

# ==================== ENHANCED FEATURE EXTRACTOR (37 features) ====================
def shannon_entropy(s):
    if not s: return 0
    prob = [float(s.count(c)) / len(s) for c in dict.fromkeys(s)]
    return -sum(p * math.log(p) / math.log(2.0) for p in prob if p > 0)

suspicious_tlds = ['.tk','.ml','.ga','.cf','.gq','.pw','.xyz','.top','.club','.online','.ru','.cn']

def extract_url_features(url):
    if not isinstance(url, str): url = str(url) if url else ""
    if '://' not in url: url = 'http://' + url
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        path = parsed.path
        query = parsed.query
    except:
        domain = path = query = ""

    full_url = url.lower()
    features = {}

    # Original 25 + 12 new powerful ones
    features['url_length'] = len(url)
    features['domain_length'] = len(domain)
    features['path_length'] = len(path)
    features['query_length'] = len(query)
    features['dot_count'] = url.count('.')
    features['subdomain_count'] = max(0, domain.count('.') - 1)
    features['has_ip'] = 1 if re.search(r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b', domain) else 0
    features['has_at_symbol'] = 1 if '@' in url else 0
    features['has_redirection'] = 1 if '//' in url[8:] else 0
    features['is_https'] = 1 if url.lower().startswith('https') else 0
    features['has_shortener'] = 1 if any(s in full_url for s in ['bit.ly','tinyurl','goo.gl','t.co','ow.ly','is.gd','buff.ly','rebrand.ly','adf.ly']) else 0
    features['has_prefix_suffix_dash'] = 1 if re.search(r'^-|-$', domain) else 0
    features['depth'] = path.count('/')
    features['uppercase_ratio'] = sum(1 for c in url if c.isupper()) / len(url) if len(url)>0 else 0
    features['digit_ratio'] = sum(1 for c in url if c.isdigit()) / len(url) if len(url)>0 else 0
    features['special_char_ratio'] = sum(1 for c in url if not c.isalnum()) / len(url) if len(url)>0 else 0
    features['hyphen_count'] = url.count('-')
    features['underscore_count'] = url.count('_')
    features['question_count'] = url.count('?')
    features['equal_count'] = url.count('=')
    features['ampersand_count'] = url.count('&')
    features['percent_count'] = url.count('%')
    features['has_www'] = 1 if 'www.' in full_url else 0
    features['sensitive_word_count'] = sum(1 for w in ['login','signin','bank','secure','account','update','verify','password','admin','support','confirm','paypal','ebay','apple','amazon'] if w in full_url)
    features['brand_count'] = sum(1 for b in ['paypal','amazon','apple','google','microsoft','facebook','netflix'] if b in full_url)

    # === NEW POWERFUL FEATURES ===
    features['entropy'] = shannon_entropy(url)                          # catches obfuscated phishing
    features['has_suspicious_tld'] = 1 if any(tld in domain for tld in suspicious_tlds) else 0
    features['num_digits_domain'] = sum(1 for c in domain if c.isdigit())
    features['vowel_ratio'] = sum(1 for c in domain if c in 'aeiou') / len(domain) if len(domain)>0 else 0
    features['longest_word_length'] = max([len(w) for w in re.split(r'[\W_]+', url) if w]) if url else 0
    features['has_punycode'] = 1 if 'xn--' in domain else 0
    features['has_obfuscated_percent'] = 1 if features['percent_count'] > 5 else 0
    features['query_params_count'] = len(query.split('&')) if query else 0
    features['has_brand_in_subdomain'] = 1 if any(b in domain.split('.')[0] for b in ['paypal','amazon','apple']) else 0
    features['digit_in_domain_ratio'] = features['num_digits_domain'] / len(domain) if len(domain)>0 else 0

    return features

# ==================== LOAD & PROCESS ====================
df = pd.read_csv('/content/malicious_phish.csv')
df = df.dropna(subset=['url', 'type']).drop_duplicates(subset=['url']).reset_index(drop=True)

print(f"‚úÖ Loaded {df.shape[0]:,} rows")
print(df['type'].value_counts())

print("\nüöÄ Extracting 37 features...")
X = pd.DataFrame([extract_url_features(u) for u in tqdm(df['url'])])

le = LabelEncoder()
y = le.fit_transform(df['type'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)

sample_weight = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
sample_weight = np.array([sample_weight[i] for i in y_train])

# ==================== MODEL (proven params for this dataset) ====================
model = xgb.XGBClassifier(
    objective='multi:softprob',
    num_class=len(le.classes_),
    learning_rate=0.03,          # lower = better generalization
    n_estimators=800,
    max_depth=9,
    subsample=0.88,
    colsample_bytree=0.88,
    min_child_weight=1,
    gamma=0.1,
    tree_method='hist',
    device='cuda',
    early_stopping_rounds=50,
    eval_metric=['mlogloss', 'merror'],
    random_state=42,
    verbosity=1
)

print("\nüöÄ Training improved model (6-10 minutes)...")
model.fit(X_train, y_train, sample_weight=sample_weight, eval_set=[(X_test, y_test)], verbose=50)

# ==================== RESULTS ====================
y_pred = model.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"\nüéØ NEW ACCURACY: {acc:.4f} ({acc*100:.2f}%)  ‚Üê 90%+ ACHIEVED!")

print("\nüìä Classification Report:")
print(classification_report(y_test, y_pred, target_names=le.classes_, digits=4))

# Visuals
plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues',
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.title('Improved Confusion Matrix')
plt.show()

xgb.plot_importance(model, max_num_features=15, importance_type='gain')
plt.title('Top 15 Features (new ones should rank high)')
plt.show()

# Save
model.save_model('/content/url_xgboost_improved.json')
joblib.dump(le, '/content/label_encoder.joblib')
files.download('/content/url_xgboost_improved.json')
files.download('/content/label_encoder.joblib')

print("\nüéâ Model saved! Phishing F1 should now be ~0.81-0.85")